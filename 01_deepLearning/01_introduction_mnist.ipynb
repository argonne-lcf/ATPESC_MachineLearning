{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Deep Learning\n",
    "Author: Marieme Ngom, adapting materials from Bethany Lusch, Asad Khan, Prasanna Balaprakash, Taylor Childers, Corey Adams, Kyle Felker, and Tanwi Mallick.\n",
    "\n",
    "This tutorial will serve as a gentle introduction to deep learning through a hands-on classification problem using the MNIST dataset.\n",
    "\n",
    "In particular, we will introduce neural networks and how to train and improve their learning capabilities. We will build up the code by hand and provide exercises. We will use theKeras API (included in the TensorFlow library)\n",
    "<img src=\"images/mnist_task.png\"  align=\"left\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import numpy \n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The MNIST dataset\n",
    "\n",
    "We will now download the dataset that contains handwritten digits. \n",
    "\n",
    "Note that downloading it the first time might take some time.\n",
    "The data is split as follows:\n",
    "- The training data x_train of size $(60000, 28, 28)$  and the corresponding classes ($\"0\", \"1\", ... \"9\"$) in y_train which is a $60000$ dimensional vector,\n",
    "- The test data x_test of size $(10000, 28*28)$  and the corresponding classes in y_test which is a $10000$ dimensional vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x_train = x_train.astype(numpy.float32)\n",
    "x_test  = x_test.astype(numpy.float32)\n",
    "\n",
    "x_train /= 255.\n",
    "x_test  /= 255.\n",
    "\n",
    "y_train = y_train.astype(numpy.int32)\n",
    "y_test  = y_test.astype(numpy.int32)\n",
    "\n",
    "print()\n",
    "print('MNIST data loaded: train:',len(x_train),'test:',len(x_test))\n",
    "print('X_train:', x_train.shape)\n",
    "print('y_train:', y_train.shape)\n",
    "print('X_test:', x_test.shape)\n",
    "print('Y_test:', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a closer look. Here are the first 10 training digits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pltsize=1\n",
    "plt.figure(figsize=(10*pltsize, pltsize))\n",
    "\n",
    "for i in range(10):\n",
    "    plt.subplot(1,10,i+1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(numpy.reshape(x_train[i,:], (28, 28)), cmap=\"gray\")\n",
    "    plt.title('Class: '+str(y_train[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generalities:\n",
    "To train our classifier, we need (besides the data):\n",
    "- A model that depend on parameters $\\mathbf{\\theta}$. Here we are going to use neural networks.\n",
    "- A loss function $J(\\mathbf{\\theta})$ to measure the capabilities of the model.\n",
    "- An optimization method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Model\n",
    "We start by training a simple linear. \n",
    "\n",
    "\n",
    " <img src=\"images/LinearModel_1.png\"  align=\"center\"/>\n",
    " \n",
    "\n",
    "\n",
    "In keras, linear layers are given by Dense() which performs a basic $xW + b$ with an optional nonlinearity applied (\"activation function\"). The Dense layer connects each input to each output with some weight parameter. They are also called \"fully connected.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearClassifier(tf.keras.models.Model):\n",
    "\n",
    "    def __init__(self, activation=tf.nn.relu):\n",
    "        tf.keras.models.Model.__init__(self)\n",
    "\n",
    "        self.layer_1 = tf.keras.layers.Dense(10, activation='softmax')\n",
    "\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # First, we need to convert the input image to a vector by using \n",
    "        #\\textbf{Flatten()}. For the mnist, it means the second dimension $28*28$ becomes $784$.\n",
    "        x = tf.keras.layers.Flatten()(inputs) \n",
    "        \n",
    "        \n",
    "        # Here, we add a \\textbf{Dense} layer that has $28 \\times 28 = 784$ input nodes \n",
    "        #(one for each pixel in the input image) and $10$ output nodes (one for each class).\n",
    "        \n",
    "        x = self.layer_1(x) \n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need:\n",
    "- A loss function $J(\\theta)$ where $\\theta$ is the list of parameters (here W and b). A popular example is the Mean Squared Error (MSE): $MSE = \\frac{1}{n}\\sum_{i =1}^n (Y_i - \\hat{Y}_i)^2$\n",
    "\n",
    "- An optimization method or optimizer such as the stochastic gradient descent (sgd) method, the Adam optimizer, RMSprop, Adagrad etc. \n",
    "\n",
    "- A metric such as \"accuracy\" to be evaluated.\n",
    "\n",
    "Here we select sparse categorical crossentropy as the loss function, the stochastic gradient descent as the optimizer, add accuracy to the list of metrics to be evaluated, and compile() the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model = LinearClassifier()\n",
    "\n",
    "linear_model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"sgd\", metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning\n",
    "Now we are ready to train our first model. A training step is comprised of:\n",
    "- A forward pass: the input is passed through the network\n",
    "- Backpropagation: A backward pass to compute the gradient $\\frac{\\partial J}{\\partial \\mathbf{W}}$ of the loss function wrt the parameters of the network\n",
    "- Weight updates $\\mathbf{W} = \\mathbf{W} - \\alpha \\frac{\\partial J}{\\partial \\mathbf{W}} $ where $\\alpha$ is the learning rate.\n",
    "The learning rate controls how quickly the model is adapted to the problem. Smaller learning rates require more training epochs given the smaller changes made to the weights each update, whereas larger learning rates result in rapid changes and require fewer training epochs:\n",
    "\n",
    "Image source: [saugatbhattarai](https://saugatbhattarai.com.np/what-is-gradient-descent-in-machine-learning/learning-rate-gradient-descent/)\n",
    "<img src=\"images/lr.jpeg\" width=\"500\" hight=\"500\" align=\"center\"/>\n",
    "\n",
    "\n",
    "- The batch size corresponds to the number of training examples in one pass (forward + backward). A smaller batch size allows the model to learn from each individual example but takes longer to train. A larger batch size trains faster but may result in the model not capturing the nuances in the data. The higher the batch size, the more memory you will require.  \n",
    "\n",
    "- The choice of batch size and learning are important for performance, generalization and accuracy in distributed deep learning.\n",
    "- An epoch means one pass through the whole training data. Using few epochs can lead to underfitting and using too many can ead to overfitting.\n",
    "\n",
    "For far more information, and some cool animations, see https://ruder.io/optimizing-gradient-descent/ or https://distill.pub/2017/momentum/.\n",
    "\n",
    "Here is a concise way to train the network. The fit function handles looping over the batches. \n",
    "You can run the code below multiple times and it will continue the training process from where it left off. If you want to start from scratch, re-initialize the model using the code a few cells ago."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "epochs = 30\n",
    "batch_size = 256\n",
    "\n",
    "history = linear_model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference\n",
    "\n",
    "For a better measure of the quality of the model, let's see the model accuracy for the test data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linscores = linear_model.evaluate(x_test, y_test, verbose=2)\n",
    "print(\"%s: %.2f%%\" % (linear_model.metrics_names[1], linscores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now take a closer look at the results.\n",
    "\n",
    "Let's define a helper function to show the failure cases of our classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_failures(predictions, trueclass=None, predictedclass=None, maxtoshow=10):\n",
    "    rounded = numpy.argmax(predictions, axis=1)\n",
    "    errors = rounded!=y_test\n",
    "    print('Showing max', maxtoshow, 'first failures. '\n",
    "          'The predicted class is shown first and the correct class in parenthesis.')\n",
    "    ii = 0\n",
    "    plt.figure(figsize=(maxtoshow, 1))\n",
    "    for i in range(x_test.shape[0]):\n",
    "        if ii>=maxtoshow:\n",
    "            break\n",
    "        if errors[i]:\n",
    "            if trueclass is not None and y_test[i] != trueclass:\n",
    "                continue\n",
    "            if predictedclass is not None and rounded[i] != predictedclass:\n",
    "                continue\n",
    "            plt.subplot(1, maxtoshow, ii+1)\n",
    "            plt.axis('off')\n",
    "            plt.imshow(x_test[i,:,:], cmap=\"gray\")\n",
    "            plt.title(\"%d (%d)\" % (rounded[i], y_test[i]))\n",
    "            ii = ii + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the first 10 test images the linear model classified to a wrong class:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linpredictions = linear_model.predict(x_test)\n",
    "\n",
    "show_failures(linpredictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- # Exercise:\n",
    "- Try changing the loss function,\n",
    "- Try changing the optimizer -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nonlinear Model\n",
    "To improve the model, we often need to add nonlinearities\n",
    "<img src=\"images/shallow_nn.png\"  align=\"left\"/>\n",
    "\n",
    "The output of the NN can be written as\n",
    "\\begin{equation}\\label{eq: NN1d}\n",
    "  \\hat{u}(x) = \\sigma_2(\\sigma_1(\\mathbf{x}\\mathbf{W}_1 + \\mathbf{b}_1)\\mathbf{W}_2 + \\mathbf{b}_2),\n",
    "\\end{equation}\n",
    "where $\\mathbf{x}$ is the input, $\\mathbf{W}_j$ are the weights of the neural network, $\\sigma_j$ the (nonlinear) activation functions, and $\\mathbf{b}_j$ its biases. The activation function introduces the nonlinearity and makes it possible to learn more complex task. Desirable properties in an activation function include being differentiable, bounded, and monotonic.\n",
    "\n",
    "\n",
    "Image source: [PragatiBaheti](https://www.v7labs.com/blog/neural-networks-activation-functions)\n",
    "<img src=\"images/activation.jpeg\"  align=\"center\"/>\n",
    "\n",
    " and more layers to obtain a deep neural network:\n",
    "<img src=\"images/deep_nn.png\"  align=\"left\"/>\n",
    "\n",
    "\n",
    "\n",
    "# Important things to know\n",
    "Deep Neural networks can be overly flexible/complicated and \"overfit\" your data. To improve the generalization of our model on previously unseen data, we employ a technique known as regularization, which constrains our optimization problem in order to discourage complex models.\n",
    "\n",
    "  - Dropout is the commonly used regularization technique. The Dropout layer randomly sets input units to 0 with a frequency of rate at each step during training time, which helps prevent overfitting.\n",
    "  - Penalizing the loss function by adding a term such as $\\lambda ||\\mathbf{W}||^2$ is alsp a commonly used regularization technique. This helps \"control\" the magnitude of the weights of the network.\n",
    "    \n",
    "<!--  <img src=\"images/test_data_rule.png\" width=\"800\" hight=\"500\" align=\"center\"/>\n",
    "  -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now implement a deep network in Keras. Dropout() performs the dropout operation mentioned earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NonlinearClassifier(tf.keras.models.Model):\n",
    "\n",
    "    def __init__(self, activation=tf.nn.tanh):\n",
    "        tf.keras.models.Model.__init__(self)\n",
    "\n",
    "        self.layer_1 = tf.keras.layers.Dense(50, activation='relu')\n",
    "        \n",
    "        self.layer_2 = tf.keras.layers.Dense(50, activation='relu')\n",
    "        self.drop_3 = tf.keras.layers.Dropout(0.2)\n",
    "        self.layer_4 = tf.keras.layers.Dense(50, activation='relu')\n",
    "        self.drop_5 = tf.keras.layers.Dropout(0.2)\n",
    "        \n",
    "        # The last layer needs to be like this:\n",
    "        self.layer_out = tf.keras.layers.Dense(10, activation='softmax')\n",
    "\n",
    "\n",
    "    def call(self, inputs):\n",
    "\n",
    "        x = tf.keras.layers.Flatten()(inputs)\n",
    "        x = self.layer_1(x)\n",
    "        \n",
    "        # The more complex version:\n",
    "        x = self.layer_2(x)\n",
    "        x = self.drop_3(x)\n",
    "        x = self.layer_4(x)\n",
    "        x = self.drop_5(x)\n",
    "        \n",
    "        x = self.layer_out(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise: \n",
    "1. Compile, fit and predict using NonlinearClassifier()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
